{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "1. [Dependency](#de)\n",
    "2. [Get Models & Export to Onnx](#gm)\n",
    "3. [TVM Config](#cfg)\n",
    "4. [Get Module](#gmo)\n",
    "5. [Auto Scheduling](#as)\n",
    "6. [Multiple Scheduling](#ms)\n",
    "7. [Evaluate Inference](#in)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"de\">Dependency"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "\n",
    "import tvm\n",
    "from tvm import relay, auto_scheduler\n",
    "from tvm.relay import data_dep_optimization as ddo\n",
    "import tvm.relay.testing\n",
    "from tvm.contrib import graph_executor"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T14:55:24.304486Z",
     "start_time": "2021-08-29T14:55:22.587830Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"gm\">Get Models & Export to Onnx"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# !pip install efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def set_efficient_model_ready(model):\n",
    "    model.set_swish(memory_efficient=False)\n",
    "    return model\n",
    "\n",
    "model_name_list = [\"resnet50\", \"resnet101\", \"efficientnet-b4\", \"efficientnet-b5\", \"efficientnet-b6\", \"efficientnet-b7\"]\n",
    "model_path_list = []\n",
    "dynamic_model_path_list = []\n",
    "model_list = []\n",
    "class_num = 801\n",
    "for name in model_name_list:\n",
    "    if name.startswith(\"res\"):\n",
    "        model = getattr(models, name)(pretrained=True)\n",
    "        num_input_fts = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_input_fts, class_num)\n",
    "        torch.manual_seed(0)\n",
    "        w = torch.randn(class_num, num_input_fts)\n",
    "        b = torch.randn(class_num)\n",
    "        model.fc.bias.data = b\n",
    "        model.fc.weight.data = w\n",
    "    else:\n",
    "        model = EfficientNet.from_pretrained(name)\n",
    "        model = set_efficient_model_ready(model)\n",
    "        num_input_fts = model._fc.in_features\n",
    "        model._fc = nn.Linear(num_input_fts, class_num)\n",
    "        torch.manual_seed(0)        \n",
    "        w = torch.randn(class_num, num_input_fts)\n",
    "        b = torch.randn(class_num)\n",
    "        model._fc.bias.data = b\n",
    "        model._fc.weight.data = w\n",
    "    print(name, w[0][:5])\n",
    "    model_path = f\"ONNX_MODELS/{name}.onnx\"\n",
    "    dynamic_model_path = f\"ONNX_MODELS/{name}_dynamic.onnx\"\n",
    "    model_path_list.append(model_path)\n",
    "    dynamic_model_path_list.append(dynamic_model_path)\n",
    "    model_list.append(model)\n",
    "    if os.path.exists(model_path):\n",
    "        continue\n",
    "    pytorch_to_onnx(model.eval(), model_path, opset_version=10)\n",
    "    pytorch_to_onnx_dynamic(model.eval(), dynamic_model_path, opset_version=10)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "resnet50 tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487])\n",
      "resnet101 tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487])\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "efficientnet-b4 tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487])\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "efficientnet-b5 tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487])\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "efficientnet-b6 tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487])\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "efficientnet-b7 tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487])\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T14:55:44.141071Z",
     "start_time": "2021-08-29T14:55:28.821669Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"cfg\">TVM Config"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Define the neural network and compilation target.\n",
    "# If the target machine supports avx512 instructions, replace the\n",
    "# \"llvm -mcpu=core-avx2\" with \"llvm -mcpu=skylake-avx512\"\n",
    "class cfg:\n",
    "    model_name = \"resnet101\"\n",
    "    model_path = f\"ONNX_MODELS/{model_name}.onnx\"\n",
    "    input_name = \"input.1\"\n",
    "    use_sparse = False\n",
    "    batch_size = 1\n",
    "    input_shape = (3, 224, 224)\n",
    "    output_shape = (batch_size, 801)\n",
    "    dtype = \"float32\"\n",
    "    layout = \"NHWC\"    \n",
    "    opset_version = 10\n",
    "    # target = tvm.target.Target(\"llvm -mcpu=core-avx2\")\n",
    "    target = \"llvm\"\n",
    "#     graph_opt_sch_file = \"...\"\n",
    "#     log_file = \"...\"\n",
    "    json_file = \"TVM_FILES/%s-%s-B%d-%s-sparse%d.json\" % (model_name, layout, batch_size, target, use_sparse)\n",
    "#     Set number of threads used for tuning based on the number ofphysical CPU cores on your machine.\n",
    "#     num_threads = 1\n",
    "    dev = tvm.device(str(target), 0)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T14:56:03.842911Z",
     "start_time": "2021-08-29T14:56:03.836981Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export Target Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "if os.path.exists(cfg.model_path):\n",
    "    print(\"onnx model already exists\")\n",
    "else:\n",
    "    model = models.resnet101(pretrained=True)\n",
    "    num_input_fts = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_input_fts, 801)\n",
    "\n",
    "    input_data = torch.randn((cfg.batch_size,)+cfg.input_shape)\n",
    "    torch.onnx.export(model, input_data, cfg.model_path, opset_version=cfg.opset_version)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "onnx model already exists\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T12:51:48.109294Z",
     "start_time": "2021-08-29T12:51:48.101653Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"gmo\">Get Module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_network_from_onnx(\n",
    "    model_path, \n",
    "    input_name=\"input.1\", \n",
    "    input_shape=(1, 3, 224, 224), \n",
    "    dtype=\"float32\"\n",
    "):\n",
    "    onnx_model = onnx.load(model_path)\n",
    "    shape_dict = {input_name: input_shape}\n",
    "    mod, params = relay.frontend.from_onnx(onnx_model, shape=shape_dict, dtype=dtype)\n",
    "    return mod, params\n",
    "    \n",
    "def convert_layout(mod):\n",
    "    desired_layouts = {'nn.conv2d': ['NHWC', 'default']}\n",
    "    seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                    relay.transform.ConvertLayout(desired_layouts)])\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        mod = seq(mod)\n",
    "    return mod\n",
    "\n",
    "def get_tvm_module_N_params(\n",
    "    model_path, \n",
    "    input_name=\"input.1\",\n",
    "    batch_size=1,\n",
    "    input_shape=(3, 224, 224),\n",
    "    layout=\"NHWC\", \n",
    "    dtype=\"float32\", \n",
    "    use_sparse=True\n",
    "):\n",
    "    \"\"\"Get the symbol definition and random weight of a network\"\"\"\n",
    "\n",
    "    data_shape = (batch_size,) + input_shape\n",
    "    # auto-scheduler prefers NHWC layout\n",
    "    mod, params = get_network_from_onnx(model_path, input_name, data_shape, dtype)\n",
    "    if layout == \"NHWC\":\n",
    "        mod = convert_layout(mod)\n",
    "#     net = mod[\"main\"]\n",
    "#     net = relay.Function(net.params, relay.nn.softmax(net.body), None, net.type_params, net.attrs)\n",
    "#     mod = tvm.IRModule.from_expr(net)\n",
    "\n",
    "    if use_sparse:\n",
    "        from tvm.topi.sparse.utils import convert_model_dense_to_sparse\n",
    "        mod, params = convert_model_dense_to_sparse(mod, params, bs_r=4, random_params=True)\n",
    "    return mod, params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extract tasks from the network\n",
    "print(\"Get module...\")\n",
    "mod, params = get_tvm_module_N_params(\n",
    "    cfg.model_path, \n",
    "    input_name=cfg.input_name,\n",
    "    batch_size=cfg.batch_size,\n",
    "    input_shape=cfg.input_shape,\n",
    "    layout=cfg.layout,\n",
    "    dtype=cfg.dtype,\n",
    "    use_sparse=cfg.use_sparse,\n",
    ")\n",
    "print(\"Extract tasks...\")\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, cfg.target)\n",
    "\n",
    "for idx, task in enumerate(tasks):\n",
    "    print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
    "    print(task.compute_dag)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T12:55:28.275859Z",
     "start_time": "2021-08-29T12:54:51.361288Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"as\">Auto Scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def run_tuning(tasks, task_weights, json_file, trials=1000, use_sparse=False):\n",
    "    print(\"Begin tuning...\")\n",
    "    tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=trials,  # change this to 20000 to achieve the best performance\n",
    "        runner=auto_scheduler.LocalRunner(repeat=10, enable_cpu_cache_flush=True),\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(json_file)],\n",
    "    )\n",
    "\n",
    "    if use_sparse:\n",
    "        from tvm.topi.sparse.utils import sparse_sketch_rules\n",
    "\n",
    "        search_policy = [\n",
    "            auto_scheduler.SketchPolicy(\n",
    "                task,\n",
    "                program_cost_model=auto_scheduler.XGBModel(),\n",
    "                init_search_callbacks=sparse_sketch_rules(),\n",
    "            )\n",
    "            for task in tasks\n",
    "        ]\n",
    "\n",
    "        tuner.tune(tune_option, search_policy=search_policy)\n",
    "    else:\n",
    "        tuner.tune(tune_option)\n",
    "\n",
    "\n",
    "# We do not run the tuning in our webpage server since it takes too long.\n",
    "# Uncomment the following line to run it by yourself.\n",
    "\n",
    "# print(cfg.json_file)\n",
    "# run_tuning(tasks, task_weights, cfg.json_file, 500)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T13:19:38.869793Z",
     "start_time": "2021-08-29T12:56:35.326432Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"ms\">Multiple Scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Extract tasks from the network\n",
    "import os\n",
    "schedule_list = [[model_name, \"NHWC\", use_sparse] for model_name in model_name_list for use_sparse in [True, False]]\n",
    "\n",
    "def multiple_scheduling(model_name, layout, use_sparse, trials=2000):\n",
    "    model_path = f\"ONNX_MODELS/{model_name}.onnx\"\n",
    "    json_fil    e = \"TVM_FILES/%s-%s-B%d-%s-sparse%d.json\" % (model_name, layout, 1, \"llvm\", use_sparse)\n",
    "    if os.path.exists(json_file):\n",
    "        return\n",
    "    mod, params = get_tvm_module_N_params(model_path, layout=layout, use_sparse=use_sparse)\n",
    "    tasks, task_weights = auto_scheduler.extract_tasks(mod[\"main\"], params, \"llvm\")\n",
    "\n",
    "    for idx, task in enumerate(tasks):\n",
    "        print(\"========== Task %d  (workload key: %s) ==========\" % (idx, task.workload_key))\n",
    "        print(task.compute_dag)\n",
    "    run_tuning(tasks, task_weights, json_file, trials)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(schedule_list)\n",
    "for m, l, s in schedule_list:\n",
    "    multiple_scheduling(m, l, s, 2000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a name=\"in\">Evaluate Inference "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import cv2\n",
    "import time\n",
    "def _calculate_dhdw_half(h, w):\n",
    "    \"\"\"Calculate difference of h or w in order to get a square \"\"\"\n",
    "    if h > w:\n",
    "        dh_half = int(0.1*h/2)\n",
    "        dw_half = int((h+2*dh_half-w)/2)\n",
    "    else:\n",
    "        dw_half = int(0.1*w/2)\n",
    "        dh_half = int((w+2*dw_half-h)/2)\n",
    "    return dh_half, dw_half\n",
    "\n",
    "def preprocess_tvm(image):\n",
    "    # 加邊框\n",
    "    h, w, c = image.shape\n",
    "    np_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    dh_half, dw_half = _calculate_dhdw_half(h, w)\n",
    "    np_img = cv2.copyMakeBorder(np_img, dh_half, dh_half, dw_half, dw_half, cv2.BORDER_REPLICATE)\n",
    "    np_img = cv2.resize(np_img, (248, 248))[12:236, 12:236]/255.0\n",
    "    return np_img.transpose(2, 0, 1).astype(cfg.dtype)[np.newaxis, :]\n",
    "\n",
    "def tvm_inference(module, img):\n",
    "    module.set_input(cfg.input_name, tvm.nd.array(img))\n",
    "    module.run()\n",
    "    tvm_output = module.get_output(0)\n",
    "    return tvm_output\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T15:21:55.924616Z",
     "start_time": "2021-07-23T15:21:05.418428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = 10\n",
    "image_path = \"/home/luhsuanwen/project/sample.jpg\"\n",
    "\n",
    "# Compile with the history best\n",
    "print(\"Compile...\")\n",
    "with auto_scheduler.ApplyHistoryBest(cfg.json_file):\n",
    "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib = relay.build(mod, target=cfg.target, params=params)\n",
    "\n",
    "# Create graph executor\n",
    "module = graph_executor.GraphModule(lib[\"default\"](cfg.dev))\n",
    "data_tvm = tvm.nd.array((np.random.uniform(size=(cfg.batch_size, )+cfg.input_shape).astype(cfg.dtype)))\n",
    "module.set_input(cfg.input_name, data_tvm)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluate inference time cost...\")\n",
    "ftimer = module.module.time_evaluator(\"run\", cfg.dev, repeat=3, min_repeat_ms=500)\n",
    "prof_res = np.array(ftimer().results) * 1e3  # convert to millisecond\n",
    "print(\"Mean inference time (std dev): %.2f ms (%.2f ms)\" % (np.mean(prof_res), np.std(prof_res)))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(n):\n",
    "    test_image = cv2.imread(image_path)\n",
    "    test_image = preprocess_tvm(test_image)\n",
    "    tvm_inference(module, test_image)\n",
    "end = time.time()\n",
    "print(cfg.json_file, (end-start)/n)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "52px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "115px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}